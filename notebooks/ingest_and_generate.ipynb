{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1712593636890
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "sys.path.append('..')\n",
        "\n",
        "import openai\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.vectorstores.azuresearch import AzureSearch\n",
        "from langchain.text_splitter import TokenTextSplitter, CharacterTextSplitter\n",
        "\n",
        "from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient\n",
        "from azure.search.documents.indexes.models import (\n",
        "    ScoringProfile,\n",
        "    SearchableField,\n",
        "    SearchField,\n",
        "    SearchFieldDataType,\n",
        "    SimpleField,\n",
        "    TextWeights,\n",
        ")\n",
        "\n",
        "from financial_qabot_table_reader.src.table2json_copy import extract_tables\n",
        "from scripts.table_and_text_parser import parse_pdf, page_text_and_tables\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv() # load environment variables from .env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "openai.api_type=\"azure\"\n",
        "openai.api_version=\"2023-05-15\"\n",
        "openai.api_base=\"https://use-gaa-openai-test1.openai.azure.com/\"\n",
        "openai.api_key=os.getenv('OPENAI_API_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# create the client class to connect to the container\n",
        "blob_service_client = BlobServiceClient.from_connection_string(os.environ['AZURE_STORAGE_CONNECTION_STRING'])\n",
        "# use the client to connect to the container\n",
        "container_client = blob_service_client.get_container_client(os.environ['AZURE_STORAGE_CONTAINER_NAME'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MDA_GWO_2023_Q4.pdf\n",
            "MFC_QPR_2023_Q4_EN.pdf\n",
            "RBC_Preview_may_1_2023.pdf\n"
          ]
        }
      ],
      "source": [
        "# list all the files/blobs in my container\n",
        "for blob in container_client.list_blobs():\n",
        "    print(blob.name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_blob_paths(container_client):\n",
        "    list_of_blob_paths = []\n",
        "    for i, blob in enumerate(container_client.list_blobs()):\n",
        "        path = \"https://\" + os.environ[\"AZURE_STORAGE_CONTAINER_ACCOUNT\"] + \\\n",
        "            \".blob.core.windows.net/\" + os.environ['AZURE_STORAGE_CONTAINER_NAME'] + \"/\" + blob.name\n",
        "        list_of_blob_paths.append(path)\n",
        "    return list_of_blob_paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "list_of_blob_paths = extract_blob_paths(container_client)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def parse_pdfs(list_of_blob_paths):\n",
        "    result_dicts = []\n",
        "    for path in list_of_blob_paths:\n",
        "        result_dicts.append(parse_pdf(path))\n",
        "    return result_dicts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "result_dicts = parse_pdfs(list_of_blob_paths)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of reports: 3\n"
          ]
        }
      ],
      "source": [
        "print('Number of reports:', len(result_dicts)) # list of list of tables for each report "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "paged_text_and_tables = []\n",
        "for result_dict in result_dicts:\n",
        "    paged_text_and_tables.append(page_text_and_tables(result_dict))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "def convert_pages_to_table_docs(paged_text_and_tables):\n",
        "\n",
        "    tables_and_text_docs = []\n",
        "\n",
        "    for report in paged_text_and_tables:\n",
        "        num_pages = max(report.keys())\n",
        "        for page_num, tables_and_text in report.items():\n",
        "            for table in tables_and_text.get('tables'):\n",
        "                if page_num > 1:\n",
        "                    metadata = ''.join(report[page_num-1].get('text')) \\\n",
        "                               + ''.join(report[page_num].get('text')) \\\n",
        "                               + ''.join(report[page_num+1].get('text'))\n",
        "                    # print('Metadata for page:', page_num, '\\n', metadata)\n",
        "                    # if metadata is None:\n",
        "                    #     metadata = ''\n",
        "                elif page_num == num_pages:\n",
        "                    metadata = ''.join(report[page_num-1].get('text')) \\\n",
        "                               + ''.join(report[page_num].get('text'))\n",
        "                    # if metadata is None:\n",
        "                    #     metadata = ''\n",
        "                    # print('Metadata for page:', page_num, '\\n', metadata)\n",
        "                else:\n",
        "                    metadata = ''.join(report[page_num+1].get('text')) \\\n",
        "                               + ''.join(report[page_num].get('text'))\n",
        "                    # if metadata is None:\n",
        "                    #     metadata = ''\n",
        "                    # print('Metadata for page:', page_num, '\\n', metadata)\n",
        "                tables_and_text_docs.append(Document(page_content=table.to_string(), \\\n",
        "                                            metadata={'text': metadata}))\n",
        "\n",
        "    return tables_and_text_docs\n",
        "    \n",
        "list_of_table_docs = convert_pages_to_table_docs(paged_text_and_tables)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "def cleanup_whitespace(s:str):\n",
        "    return re.sub(\"\\s+\", \" \", s)\n",
        "\n",
        "def preprocess_docs(lang_chunks):\n",
        "    for doc in lang_chunks:\n",
        "        doc.page_content = cleanup_whitespace(doc.page_content)\n",
        "    return lang_chunks\n",
        "\n",
        "lang_chunks_clean = preprocess_docs(list_of_table_docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "# text_splitter = TokenTextSplitter(chunk_size=400, chunk_overlap=20)\n",
        "text_splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=20)\n",
        "lang_chunks = text_splitter.split_documents(list_of_table_docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "# for i, chunk in enumerate(lang_chunks):\n",
        "#     print('Chunk:', i, 'Chunk length:', len(chunk.page_content), '\\n', chunk.page_content, '\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/anaconda/envs/finqa/lib/python3.8/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.embeddings.openai.OpenAIEmbeddings` was deprecated in langchain-community 0.1.0 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
            "  warn_deprecated(\n"
          ]
        }
      ],
      "source": [
        "embeddings = OpenAIEmbeddings(\n",
        "    deployment='text-embedding-ada-002-v2',\n",
        "    openai_api_base=os.environ['OPENAI_API_BASE'],\n",
        "    openai_api_type=os.environ['OPENAI_API_TYPE'],\n",
        "    openai_api_key=os.environ['OPENAI_API_KEY'],\n",
        "    openai_api_version=os.environ['OPENAI_API_VERSION'],\n",
        "    # chunk_size = 1\n",
        "    )\n",
        "\n",
        "embedding_function=embeddings.embed_query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "fields = [\n",
        "    # SimpleField(\n",
        "    #     name=\"id\",\n",
        "    #     type=SearchFieldDataType.String,\n",
        "    #     key=True,\n",
        "    #     filterable=True,\n",
        "    # ),\n",
        "    # SearchableField(\n",
        "    #     name=\"content\",\n",
        "    #     type=SearchFieldDataType.String,\n",
        "    #     searchable=True,\n",
        "    # ),\n",
        "    # SearchField(\n",
        "    #     name=\"content_vector\",\n",
        "    #     type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
        "    #     searchable=True,\n",
        "    #     vector_search_dimensions=len(embedding_function(\"Text\")),\n",
        "    #     vector_search_configuration=\"default\", ##the \"default\" option is explained below\n",
        "    # ),\n",
        "    SearchableField(\n",
        "        name=\"metadata\",\n",
        "        type=SearchFieldDataType.String,\n",
        "        searchable=True,\n",
        "        filterable=True,\n",
        "    ),\n",
        "    # # Additional field to store the title\n",
        "    # SimpleField(\n",
        "    #     name=\"policy_number\",\n",
        "    #     type=SearchFieldDataType.String,\n",
        "    #     filterable=True,\n",
        "    #     searchable=True,\n",
        "    # )\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "# azure_ai_search_name = 'nlp-ai-search1'\n",
        "azure_search_endpoint = \"https://\" + os.environ['AZURE_AI_SEARCH_SERVICE_NAME'] + \".search.windows.net\"\n",
        "\n",
        "acs_vector_store = AzureSearch(\n",
        "    azure_search_endpoint=azure_search_endpoint,\n",
        "    azure_search_key=os.environ['AZURE_AI_SEARCH_KEY'],\n",
        "    index_name=os.environ['AZURE_AI_SEARCH_INDEX_NAME'],\n",
        "    embedding_function=embedding_function,\n",
        "    fields=fields,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # [START delete_index]\n",
        "# client = SearchIndexClient(service_endpoint, AzureKeyCredential(key))\n",
        "# name = \"hotels\"\n",
        "# client.delete_index(name)\n",
        "# # [END delete_index]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "# we need to clear out exising index before adding documents\n",
        "# acs_vector_store.add_documents(documents=lang_chunks_clean)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chunk 0 \n",
            " 0 None None None None None None 0 Core earnings 1 Asia $ 564 $ 522 $ 496 $ 2,048 $ 1,812 2 Canada 352 408 296 1,487 1,387 3 U.S. 474 442 408 1,759 1,566 4 Global Wealth and Asset Management 353 361 274 1,321 1,299 5 Corporate and Other 30 10 69 69 (263) 6 Total core earnings $ 1,773 $ 1,743 $ 1,543 $ 6,684 $ 5,801 7 Items excluded from core earnings: 8 Market experience gains (losses) (133) (1,022) (655) (1,790) (2,585) 9 Change in actuarial methods and assumptions that flow directly through income 119 (14) - 105 26 10 Restructuring charge (36) - - (36) - 11 Reinsurance transactions, tax-related items and other (64) 306 340 140 256 12 Net income attributed to shareholders / Transitional $ 1,659 $ 1,013 $ 1,228 $ 5,103 $ 3,498 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# input_type = 'List of JSONs'\n",
        "# input_type = 'List of DataFrames'\n",
        "\n",
        "query = \"\"\"\n",
        "What are the full year core earnings?\n",
        "\"\"\"\n",
        "\n",
        "returned_chunks = acs_vector_store.similarity_search(\n",
        "    query=query,\n",
        "    k=1,\n",
        "    search_type=\"similarity\",\n",
        ")\n",
        "\n",
        "for i, chunk in enumerate(returned_chunks):\n",
        "    print('Chunk', i, '\\n', chunk.page_content, '\\n')\n",
        "\n",
        "input = []\n",
        "for chunk in returned_chunks:\n",
        "    input.append(chunk.page_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The full year core earnings are $6,684.'"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt = f\"\"\"\n",
        "Answer the QUESTION enclosed in the dollar signs (i.e, $) from the data enclosed in triple backticks (i.e., ```).\n",
        "Do not answer from memory. If you do not know an answer, just say I do not know.\n",
        "\n",
        "QUESTION: \n",
        "$\n",
        "{query}\n",
        "$\n",
        "\n",
        "```\n",
        "{input}\n",
        "```\n",
        "\"\"\"\n",
        "\n",
        "message_text = [{\"role\":\"system\",\"content\":\"You are an AI assistant that helps people find information.\"},\n",
        "{\"role\": \"user\",\"content\": prompt}]\n",
        "\n",
        "completion = openai.ChatCompletion.create(\n",
        "  engine=\"gpt-4-32k\", # model = \"deployment_name\" # try gpt-4\n",
        "  messages = message_text,\n",
        "  temperature=0.7, # 0.7\n",
        "  max_tokens=800,\n",
        "  top_p=0.95\n",
        ")\n",
        "\n",
        "# print(prompt)\n",
        "completion.get('choices')[0].get('message').get('content')"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "myenv"
    },
    "kernelspec": {
      "display_name": "finqa",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.19"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

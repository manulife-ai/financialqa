import os
import time
import copy
import logging
import argparse

from PIL import Image
from PyPDF2 import PdfReader, PdfWriter
from pdf2image import convert_from_path
from langchain.docstore.document import Document
from langchain_openai import AzureOpenAIEmbeddings
from langchain_community.vectorstores.azuresearch import AzureSearch
from langchain.text_splitter import TokenTextSplitter, CharacterTextSplitter

from transformers import Pix2StructProcessor, Pix2StructForConditionalGeneration
# from src.financialqa.ingestion.models.model_base_decoder import infer_base_decoder

from azure.storage.blob import BlobServiceClient
from azure.identity import DefaultAzureCredential
from azure.core.credentials import AzureKeyCredential
from azure.search.documents.indexes import SearchIndexClient
from azure.search.documents.indexes.models import (
    SearchableField,
    SearchField,
    SearchFieldDataType,
    SimpleField,
    VectorSearch,
    HnswParameters,
    VectorSearchProfile,
    VectorSearchAlgorithmKind,
    HnswAlgorithmConfiguration,
    VectorSearchAlgorithmMetric,
)

from dotenv import load_dotenv
load_dotenv(override=True)

from .helper import preprocess_text
from .parser import extract_pdf_contents, page_pdf_contents

'''
To-do's:
'''

class IngestionPipeline:
    """Ingest multi-structured PDF contents into a search index.

    Extract text, tables, and figures from PDFs, convert figures to 
    tables, create and generate embeddings of Document objects of 
    extracted PDF contents, and upload Documents to a search index.

    Attributes:
        blob_storage_container (ContainerClient): The client for the 
            blob storage container.
        search_client (SearchIndexClient): The client for the Azure AI 
            Search service.
        embedding_model (AzureOpenAIEmbeddings): The model used for 
            generating vector embeddings.
        local_pdfs_folder (str): Local folder containing (or where to 
            store) PDF files.
    """
    
    def __init__(self):
        self.configure_logging()
        self._load_api_vars()
        self.blob_storage_container = self._get_blob_container_client()
        self.search_client = self._get_search_client()
        self.embedding_model = self._get_embedding_model()
        self.local_pdfs_folder = 'data/'

    def configure_logging(self) -> None:
        """
        Configure logging for the ingestion pipeline.

        Returns:
            None
        """
        logger = logging.getLogger(
            'azure.core.pipeline.policies.http_logging_policy'
        )  
        logger.setLevel(logging.CRITICAL)
        httpx_logger = logging.getLogger('httpx')  
        httpx_logger.setLevel(logging.WARNING)  # Suppress Azure INFO logs 
        logging.basicConfig(        
            level=logging.INFO,    
            format="%(asctime)s %(levelname)s %(name)s line %(lineno)d  %(message)s",    
            datefmt="%H:%M:%S",
            force=True,
        )  
        self.logger = logging.getLogger(__name__)  
        self.logger.info("Configured logging")

    def extract_pdfs_from_blob(
            self,
            overwrite_files: bool = False,
        ) -> None:
        """
        Read PDF files from an Azure Blob storage container and save to 
        local disk.

        Args:
            overwrite_files (bool): Whether to overwrite existing files 
                (default: False).

        Returns:
            None
        """
        self.logger.info(
            "Reading files from Azure Blob storage container '{0}'...".format(
                self.azure_storage_container_name,
        ))
        for blob in self.blob_storage_container.list_blobs():
            if blob.name.endswith('.pdf'):
                blob_name = blob.name
                blob_client = self.blob_storage_container.get_blob_client(blob_name)
                pdf_file_path = os.path.join(self.local_pdfs_folder, blob_name)
                os.makedirs(
                    os.path.dirname(pdf_file_path), 
                    exist_ok=True,
                )
                if os.path.isfile(pdf_file_path) and not overwrite_files:
                    self.logger.info("File '{0}' already exists in {1}".format(
                        blob_name,
                        self.local_pdfs_folder,
                    ))
                    continue
                with open(pdf_file_path, "wb") as pdf_file:
                    pdf_file.write(blob_client.download_blob().readall())
                self.logger.info("Downloaded file '{0}' to {1}".format(
                    blob_name,
                    pdf_file_path,
                ))
    
    def crop_images_from_pdfs(
            self, 
            pdf_pages_dict: dict[str, dict],
            save_as_jpg: bool = True,
        ) -> None:
        """
        Crop and save images from PDFs using PyPDF2 based on bounding 
        box coordinates generated by Azure Document Intelligence.

        Args:
            pdf_pages_dict (dict): A dictionary containing extracted PDF 
                contents.
            save_as_jpg (bool): Whether to save cropped images as JPEG 
                (default: True).
        
        Returns:
            None
        """
        for pdf_name, pdf_contents in pdf_pages_dict.items():
            pdf_pages = pdf_contents.get('pages')
            for page_num, page_dict in pdf_pages.items():
                if 'figures' not in page_dict:
                    continue
                self.logger.info(
                    f"Processing figures on page {page_num} of '{pdf_name}'..."
                )
                pdf_figures = page_dict.get('figures')
                pdf_file = os.path.join('../data', pdf_name + '.pdf')
                reader = PdfReader(pdf_file)
                page = reader.pages[page_num-1]
                for figure_name, figure_items in pdf_figures.items():
                    bounding_regions_polygon = figure_items.get('bounding_regions_polygon')
                    # found this 72 scaling factor here:
                    # https://github.com/microsoft/Form-Recognizer-Toolkit/blob/main/SampleCode/Python/sample_figure_understanding.ipynb
                    bounding_regions_polygon = [72 * polygon for polygon in bounding_regions_polygon]
                    x1, y1, x2, y2, x3, y3, x4, y4 = bounding_regions_polygon
                    page_mediabox_upper_left = copy.deepcopy(page.mediabox.upper_left)
                    page_mediabox_lower_right = copy.deepcopy(page.mediabox.lower_right)
                    new_upper_left = (
                        page.mediabox.upper_left[0].as_numeric() + x1, 
                        page.mediabox.upper_left[1].as_numeric() - y1
                    )
                    new_lower_right = (
                        page.mediabox.upper_left[0].as_numeric() + x3, 
                        page.mediabox.upper_left[1].as_numeric() - y3
                    )
                    page.mediabox.upper_left = new_upper_left
                    page.mediabox.lower_right = new_lower_right
                    writer = PdfWriter()  #  Find a workaround to reinitalizing
                    writer.add_page(page)
                    page.mediabox.upper_left = page_mediabox_upper_left
                    page.mediabox.lower_right = page_mediabox_lower_right
                    pdf_output_dir = os.path.join(
                        self.local_pdfs_folder, 
                        'outputs', 
                        pdf_name
                    )
                    if not os.path.isdir(pdf_output_dir):
                        os.makedirs(pdf_output_dir)
                    pdf_file_path = os.path.join(
                        pdf_output_dir, figure_name + '.pdf',
                    )
                    pdf_pages.get(page_num).get('figures').get(figure_name).\
                        update({'pdf_file_path': pdf_file_path})
                    with open(pdf_file_path, 'wb') as out_f:
                        writer.write(out_f)
                        self.logger.info(
                            "Saving figure '{0}' as PDF to {1}".format(
                                figure_name,
                                pdf_output_dir,
                            ))
                    if save_as_jpg:
                        jpg_output_folder = os.path.join(
                            '../data', 'outputs', pdf_name, 'jpg_outputs',
                        )
                        if not os.path.isdir(jpg_output_folder):   
                            os.makedirs(jpg_output_folder)
                        images = convert_from_path(
                            pdf_file_path,
                            fmt='JPEG',
                            output_folder=jpg_output_folder,
                        )
                        jpg_file_path = os.path.join(
                            jpg_output_folder, 
                            figure_name + '.jpg',
                        )
                        pdf_pages.get(page_num).get('figures').get(figure_name).\
                            update({'jpg_file_path': jpg_file_path})
                        for img in images:
                            self.logger.info(
                                "Saving figure '{0}' as JPEG to {1}".format(
                                    figure_name,
                                    jpg_output_folder,
                                ))
                            img.save(fp=jpg_file_path)

    def generate_chartvlm_output(
            self,
            model_path: str,
            image_path: str,
        ) -> str:
        """
        Generate ChartVLM output based on model and image path.
        
        Args:
            model_path (str): Path to ChartVLM model.
            image_path (str): Path to image file.
        
        Returns:
            str: ChartVLM output.
        """
        self.logger.info(
            f'Generating ChartVLM output for figure with image path {image_path}...'
        )
        img = Image.open(image_path).convert("RGB")
        chartvlm_decoder_output = infer_base_decoder(
            img, 
            model_path, 
            title_type=False,
        )
        self.logger.info(
            f'ChartVLM output for figure with image path {image_path}:\n{chartvlm_decoder_output}'
        )
        return chartvlm_decoder_output
    
    def generate_deplot_output(
            self, 
            image_path: str,
        ) -> str:
        """
        Generate DePlot output based on image path.
        
        Args:
            image_path (str): Path to image file.
            
        Returns:
            str: DePlot output.
        """
        pix2struct_processor = Pix2StructProcessor.from_pretrained('google/deplot')
        pix2struct_model = Pix2StructForConditionalGeneration.from_pretrained('google/deplot')
        self.logger.info(
            f'Generating DePlot output for figure with image path {image_path}...'
        )
        image = Image.open(fp=image_path)
        inputs = pix2struct_processor(
            images=image, 
            text="Generate underlying data table of the figure below:", 
            return_tensors="pt"
        )
        predictions = pix2struct_model.generate(**inputs, max_new_tokens=512)
        deplot_output = pix2struct_processor.decode(
            predictions[0], 
            skip_special_tokens=True
        )
        self.logger.info(
            f'DePlot output for figure with image path {image_path}:\n{deplot_output}'
        )
        return deplot_output

    def generate_table_from_chart(
            self,
            pdf_pages_dict: dict[str, dict],
            chart_model: str = 'deplot',
            chart_model_path: str = '',
        ) -> None:
        """
        Generate linearized table from chart figure using specified model.
        
        Args:
            pdf_pages_dict (dict): A dictionary containing extracted PDF 
                contents.
            chart_model (str): Model to use for converting chart to 
                table (default: 'deplot').
            chart_model_path (str): Path to chart model (default: None).
            
        Returns:
            None
        """
        for pdf_name, pdf_items in pdf_pages_dict.items():
            pages = pdf_items.get('pages')
            for page_num, page_content in pages.items():
                if 'figures' not in page_content.keys():
                    continue
                for figure_name, figure_contents in page_content.get('figures').items():
                    image_path = figure_contents.get('jpg_file_path')
                    if chart_model == 'chartvlm':
                        chart_to_table_output = self.generate_chartvlm_output(
                            chart_model_path,
                            image_path,
                        )
                    elif chart_model == 'deplot':
                        chart_to_table_output = self.generate_deplot_output(
                            image_path,
                        )
                    figure_contents.update({'chart_table': chart_to_table_output})

    def convert_paged_pdf_contents_to_docs(
            self, 
            pdf_pages_dict,
            convert_tables: bool = True,
            convert_charts: bool = True,
        ) -> tuple:
        """
        Create Document objects from extracted PDF contents.

        Args:
            pdf_pages_dict (dict): A dictionary containing extracted PDF 
                contents.
            convert_tables (bool): Whether to convert extracted tables 
                to Document objects (default: True).
            convert_charts (bool): Whether to convert extracted charts 
                to Document objects (default: True).
        
        Returns:
            tuple: A tuple containing three lists: text_docs, table_docs, 
                and chart_docs.
        """
        text_docs = []
        table_docs = []
        chart_docs = []
        for pdf_name, pdf_items in pdf_pages_dict.items():
            self.logger.info(
                f"Converting extracted '{pdf_name}' contents to Document objects..."
            )
            company_name = pdf_items.get('company_name')
            report_quarter = pdf_items.get('report_quarter')
            pages = pdf_items.get('pages')
            for page_num, page_content in pages.items():
                page_titles = ''
                page_headers = ''
                page_footers = ''
                section_headers = ''
                if 'title' in page_content.keys():
                    page_titles = ', '.join(page_content.get('title'))
                if 'sectionHeading' in page_content.keys():
                    section_headers = ', '.join(page_content.get('sectionHeading'))
                if 'pageHeader' in page_content.keys():
                    page_headers = ', '.join(page_content.get('pageHeader'))
                if 'pageFooter' in page_content.keys():
                    page_footers = ', '.join(page_content.get('pageFooter'))
                # Convert text content by default
                text_content = preprocess_text(' '.join(page_content.get('text')))
                text_docs.append(
                    Document(
                        page_content=text_content,
                        metadata={
                            'page_num': page_num,
                            'pdf_name': pdf_name,
                            'company_name': company_name,
                            'report_quarter': report_quarter,
                            'page_titles': page_titles,
                            'page_headers': page_headers,
                            'section_headers': section_headers,
                            'page_footers': page_footers,
                        }))
                if convert_tables:
                    if 'tables' not in page_content.keys():
                        continue
                    for table in page_content.get('tables'):
                        table_docs.append(
                            Document(
                                page_content=str(table),
                                metadata={
                                    'text': text_content, 
                                    'pdf_name': pdf_name,
                                    'page_num': page_num,
                                    'company_name': company_name,
                                    'report_quarter': report_quarter,
                                    'page_titles': page_titles,
                                    'page_headers': page_headers,
                                    'section_headers': section_headers,
                                    'page_footers': page_footers,
                                }))
                if convert_charts:
                    if 'figures' not in page_content.keys():
                        continue
                    for figure_name, figure_contents in page_content.get('figures').items():
                        if 'chart_table' in figure_contents.keys():
                            chart_table = figure_contents.get('chart_table')
                        else:
                            self.logger.info(
                                f"Missing chart table for {figure_name} of '{pdf_name}'")
                            break
                        chart_docs.append(
                            Document(
                                page_content=chart_table.strip(),
                                metadata={
                                    'page_num': page_num,
                                    'pdf_name': pdf_name,
                                    'company_name': company_name,
                                    'report_quarter': report_quarter,
                                    'page_titles': page_titles,
                                    'page_headers': page_headers,
                                    'section_headers': section_headers,
                                    'page_footers': page_footers,
                                }))
            self.logger.info(
                "Created {0} text, {1} table, and {2} chart Documents for file '{3}'".format(
                    len(text_docs), 
                    len(table_docs), 
                    len(chart_docs),
                    pdf_name,
                ))
        return text_docs, table_docs, chart_docs
    
    def chunk_docs(
            self,
            docs: list[Document],
            chunk_size: int = 400
        ) -> list[Document]:
        """
        Chunk list of Document objects.

        Args:
            docs (list): List of Document objects.
            chunk_size (int): Size of chunks to split Documents into 
                (default: 400).
        
        Returns:
            list: List of chunked Document objects.
        """
        text_splitter = TokenTextSplitter(chunk_size=chunk_size, chunk_overlap=0)
        # text_splitter = CharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=0)
        doc_chunks = text_splitter.split_documents(docs)
        self.logger.info(
            "Chunked {0} Documents to {1} Documents using a chunk size of {2}".format(
                len(docs), len(doc_chunks), chunk_size))
        return doc_chunks
    
    def get_search_index(
            self, 
            upload_docs: list[Document] = None,
            upload_docs_in_batches: bool = False,
            batch_size: int = 50,
            overwrite_index: bool = False,
        ) -> AzureSearch:
        """
        Get an Azure AI Search index with an option to upload Documents 
        to it.

        Args:
            upload_docs (list): List of Document objects to upload to 
                index (default: None).
            upload_docs_in_batches (bool): Whether to upload Documents 
                in batches (default: False).
            batch_size (int): Number of Documents to upload per batch 
                (default: 50).
            overwrite_index (bool): Whether to overwrite index if it 
                exists (default: False).
        
        Returns:
            ai_search_index (AzureSearch): Azure AI Search index object.
        """
        self.logger.info("Getting index '{0}' from AI Search service '{1}'...".\
            format(
                self.azure_search_index_name,
                self.azure_search_service_name,
            ))
        try:
            self.search_client.get_index(self.azure_search_index_name)
        except:
            self.logger.info(
                "Did not find index '{0}' in service {1}".\
                    format(
                        self.azure_search_index_name, 
                        self.azure_search_service_name
                    ))
            index_exists = 0
        else:
            self.logger.info(
                "Found existing index '{0}' in search service '{1}'".\
                    format(
                        self.azure_search_index_name, 
                        self.azure_search_service_name
                    ))
            index_exists = 1
        finally:
            # See default field names here: https://python.langchain.com/docs/integrations/vectorstores/azuresearch/
            fields = [
                SimpleField(
                    name="id",
                    type=SearchFieldDataType.String,
                    key=True,
                    filterable=True,
                ),
                SearchableField(
                    name="content",
                    type=SearchFieldDataType.String,
                    searchable=True,
                ),
                SearchField(
                    name="content_vector",
                    type=SearchFieldDataType.Collection(SearchFieldDataType.Single),
                    searchable=True,
                    vector_search_dimensions=len(
                        self.embedding_model.embed_query("Text")
                    ),
                    vector_search_profile_name="finqaHnsw",
                ),
                SearchableField(
                    name="metadata",
                    type=SearchFieldDataType.String,
                    searchable=True,
                    filterable=True,
                ),
                SimpleField(
                    name="company_name",
                    type=SearchFieldDataType.String,
                    filterable=True,
                    searchable=True,
                ),
                SimpleField(
                    name="report_quarter",
                    type=SearchFieldDataType.String,
                    filterable=True,
                    searchable=True,
                ),
                SimpleField(
                    name="page_titles",
                    type=SearchFieldDataType.String,
                    searchable=True,
                ),
                SimpleField(
                    name="page_headers",
                    type=SearchFieldDataType.String,
                    searchable=True,
                ),
                SimpleField(
                    name="page_footers",
                    type=SearchFieldDataType.String,
                    searchable=True,
                ),
                SimpleField(
                    name="section_headers",
                    type=SearchFieldDataType.String,
                    searchable=True,
                ),
            ]
            vector_search = VectorSearch(
                algorithms=[
                    HnswAlgorithmConfiguration(
                        name="finqaHnsw",
                        kind=VectorSearchAlgorithmKind.HNSW,
                        parameters=HnswParameters(
                            m=4,
                            ef_construction=400,
                            ef_search=500,
                            metric=VectorSearchAlgorithmMetric.COSINE,
                        ),
                    )],
                profiles=[
                    VectorSearchProfile(
                        name="finqaHnsw",
                        algorithm_configuration_name="finqaHnsw",
                    ),
                ])
            if overwrite_index:
                if index_exists:
                    self.logger.info("Overwriting index '{0}' in search service '{1}'...".\
                        format(
                            self.azure_search_index_name, 
                            self.azure_search_service_name
                        ))
                    self.search_client.delete_index(self.azure_search_index_name)
                else:
                    self.logger.info("Did not find index to overwrite")
            ai_search_index = AzureSearch(
                azure_search_endpoint=self.azure_search_endpoint,
                azure_search_key=self.azure_search_key,
                index_name=self.azure_search_index_name,
                embedding_function=self.embedding_model.embed_query,
                fields=fields,
                additional_search_client_options={"retry_total": 4},
                vector_search=vector_search,
            )
            if upload_docs is not None:
                if upload_docs_in_batches:
                    doc_batches = [
                        upload_docs[i:i+batch_size] for i in range(
                            0, len(upload_docs), batch_size)
                        ]
                    tot_time = time.time()
                    for doc_batch in doc_batches:
                        batch_time = time.time()
                        try:
                            self.logger.info("Attempting to upload batch of {0} Documents to index '{1}'...".\
                                format(
                                    len(doc_batch), 
                                    self.azure_search_index_name
                                ))
                            ai_search_index.add_documents(documents=upload_docs)
                        except: 
                            self.logger.info("Unable to upload batch of {0} Documents to index '{1}'. Trying again".\
                                format(
                                    len(doc_batch), 
                                    self.azure_search_index_name, 
                                ))
                        else:
                            self.logger.info("Successfully uploaded batch of {0} Documents to index '{1}' in {2} seconds".\
                                format(
                                    len(doc_batch), 
                                    self.azure_search_index_name, 
                                    round(time.time() - batch_time)
                                ))
                    self.logger.info("Successfully uploaded a total of {0} Documents to index '{1}' in {2} seconds".\
                        format(
                            len(upload_docs), 
                            self.azure_search_index_name, 
                            round(time.time() - tot_time)
                        ))
                else:
                    self.logger.info("Attempting to upload {0} Documents to index '{1}'...".\
                        format(
                            len(upload_docs), 
                            self.azure_search_index_name
                        ))
                    t = time.time()
                    ai_search_index.add_documents(documents=upload_docs)
                    self.logger.info("Successfully uploaded {0} Documents to index '{1}' in {2} seconds".\
                        format(
                            len(upload_docs), 
                            self.azure_search_index_name, 
                            round(time.time() - t)
                        ))
        return ai_search_index

    def ingest_pdfs(
            self, 
            extract_pdfs_from_blob: bool = True,
            convert_chart_to_table: bool = False,
            chart_to_table_model: str = 'deplot',
            chart_to_table_model_path: str = '',
            upload_docs_in_batches: bool = True,
            batch_size: int = 50,
            overwrite_index: bool = False,
        ) -> None:
        """
        Extract text, tables, and figures from PDFs, create, chunk and 
        generate embeddings of Document objects of PDF contents, and 
        upload Documents to an index.

        Args:
            extract_pdfs_from_blob (str): Whether to extract PDFs from 
                Blob storage (default: True).
            convert_chart_to_table (bool): Whether to convert chart to 
                tabular form (default: False).
            chart_to_table_model (str): Model used to convert chart to 
                tabular form (default: 'deplot').
            chart_to_table_model_path (str): Chart to table model path 
                (default: None).
            upload_docs_in_batches (bool): Whether to upload Documents 
                to index in batches (default: True).
            batch_size (int): Number of Documents to upload per batch 
                (default: 50).
            overwrite_index (bool): Whether to overwrite index if it 
                exists (default: False).
            
        Returns:
            None
        """
        if extract_pdfs_from_blob:
            self.extract_pdfs_from_blob()
        pdf_contents = extract_pdf_contents(self.local_pdfs_folder)
        pdf_pages_dict = page_pdf_contents(pdf_contents)
        if convert_chart_to_table:
            self.crop_images_from_pdfs(
                pdf_pages_dict, 
                save_as_jpg=True,
            )
            self.generate_table_from_chart(
                pdf_pages_dict,
                chart_model=chart_to_table_model,
                chart_model_path=chart_to_table_model_path,
            )
        text_docs, table_docs, chart_docs = \
            self.convert_paged_pdf_contents_to_docs(
                pdf_pages_dict,
                convert_charts=convert_chart_to_table,
            )
        text_doc_chunks = self.chunk_docs(text_docs)
        table_doc_chunks = self.chunk_docs(table_docs)
        chart_doc_chunks = self.chunk_docs(chart_docs)
        combined_docs = text_doc_chunks + table_doc_chunks + chart_doc_chunks
        self.get_search_index(
            upload_docs=combined_docs,
            upload_docs_in_batches=upload_docs_in_batches,
            batch_size=batch_size,
            overwrite_index=overwrite_index,
        )

    def _get_blob_container_client(self) -> BlobServiceClient:
        """
        Instantiate Azure Blob Storage container client.

        Returns:
            ContainerClient: The client for the Azure Blob storage 
                container.
        """
        self.logger.info(
                "Getting Azure Blob Storage container client '{0}' from account '{1}'...".format(
                    self.azure_storage_container_name,
                    self.azure_storage_container_account,
                ))
        blob_service_client = \
            BlobServiceClient.from_connection_string(
                self.azure_storage_connection_string
            )
        container_client = \
            blob_service_client.get_container_client(
                self.azure_storage_container_name
            )
        return container_client

    def _get_search_client(self) -> SearchIndexClient:
        """
        Instantiate Azure AI Search client.

        Returns:
            SearchIndexClient: The client for the Azure AI Search 
                service.
        """
        self.logger.info("Getting Azure AI Search client from service '{0}'...".format(
            self.azure_search_service_name
            ))
        azure_search_endpoint = "https://" + self.azure_search_service_name \
                                    + ".search.windows.net"
        search_client = SearchIndexClient(
            azure_search_endpoint, 
            AzureKeyCredential(self.azure_search_key),
        )
        return search_client

    def _get_embedding_model(self) -> AzureOpenAIEmbeddings:
        """
        Instantiate Azure OpenAI embedding model.

        Returns:
            AzureOpenAIEmbeddings: The model used for generating vector 
                embeddings.
        """
        self.logger.info(
            "Getting OpenAI embedding model '{0}' from endpoint {1}".format(
                self.azure_openai_embedding_model,
                self.azure_openai_endpoint,
            ))
        embeddings = AzureOpenAIEmbeddings(
            deployment=self.azure_openai_embedding_model,
            azure_endpoint=self.azure_openai_endpoint,
            openai_api_version=self.azure_openai_version,
            openai_api_key=self.azure_openai_key,
        )
        return embeddings
    
    def _load_api_vars(self) -> None:
        """
        Load API variables from environment variables.

        Returns:
            None
        """
        self.azure_openai_key = os.getenv('AZURE_OPENAI_KEY')
        self.azure_openai_type = os.getenv('AZURE_OPENAI_TYPE')
        self.azure_openai_version = os.getenv('AZURE_OPENAI_VERSION')
        self.azure_openai_endpoint = os.getenv('AZURE_OPENAI_ENDPOINT')
        self.azure_openai_embedding_model = os.getenv('AZURE_OPENAI_EMBEDDING_MODEL')
        self.azure_search_key = os.getenv('AZURE_AI_SEARCH_KEY')
        self.azure_search_index_name = os.getenv('AZURE_AI_SEARCH_INDEX_NAME')
        self.azure_search_service_name = os.getenv('AZURE_AI_SEARCH_SERVICE_NAME')
        self.azure_storage_container_name = os.getenv('AZURE_STORAGE_CONTAINER_NAME')
        self.azure_storage_container_account = os.getenv('AZURE_STORAGE_CONTAINER_ACCOUNT')
        self.azure_storage_connection_string = os.getenv('AZURE_STORAGE_CONNECTION_STRING')
        self.azure_search_endpoint = "https://" + self.azure_search_service_name + ".search.windows.net"
        self.logger.info("Loaded API variables")


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--extract_pdfs_from_blob",
        action='store_true',
        help="Whether to extract PDFs from Azure Blob storage and save to local disk (default: False)",
        default=False,
    )
    parser.add_argument(
        "--convert_chart_to_table",
        help="Whether to convert chart figures to tabular form (default: False)",
        action='store_true',
        default=False,
    )
    parser.add_argument(
        "--chart_to_table_model",
        type=str,
        help="Chart model used to convert the chart to tabular form (default: 'deplot')",
        default='deplot',
    )
    parser.add_argument(
        "--chart_to_table_model_path",
        type=str,
        help="Chart to table model path (default: None)",
        default='',
    )
    parser.add_argument(
        "--upload_docs_in_batches",
        help="Whether to upload Documents to index in batches (default: True)",
        action='store_true',
        default=True,
    )
    parser.add_argument(
        "--batch_size",
        type=int,
        help="Batch size to use when uploading Documents to index (default: 50)",
        default=50,
    )
    parser.add_argument(
        "--overwrite_index",
        help="Whether to overwrite search index if it exists (default: False)",
        action='store_true',
        default=False,
    )
    args = parser.parse_args()
    ingestion_pipeline = IngestionPipeline()
    ingestion_pipeline.ingest_pdfs(
        extract_pdfs_from_blob=args.extract_pdfs_from_blob, 
        convert_chart_to_table=args.convert_chart_to_table,
        chart_to_table_model=args.chart_to_table_model,
        chart_to_table_model_path=args.chart_to_table_model_path,
        upload_docs_in_batches=args.upload_docs_in_batches,
        batch_size=args.batch_size,
        overwrite_index=args.overwrite_index,
    )